name: PO Pipeline (Vercel Worker + Idempotent Reports)

on:
  # Triggered by your external caller (e.g., the worker) via repository_dispatch
  repository_dispatch:
    types: [po_review_request]
  # Manual button in Actions tab
  workflow_dispatch: {}

permissions:
  contents: write  # allow committing reports/ back to the repo

jobs:
  po-review:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install pipeline dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install --no-cache-dir -r requirements.txt; fi

      - name: Download inputs from worker (optional – will skip if not available)
        env:
          WORKER_URL: ${{ secrets.WORKER_URL }}
          WORKER_TOKEN: ${{ secrets.WORKER_TOKEN }}
          JOB_ID: ${{ github.event.client_payload.jobId }}
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p inputs
          if [ -n "${WORKER_URL:-}" ] && [ -n "${WORKER_TOKEN:-}" ] && [ -n "${JOB_ID:-}" ]; then
            for name in po pi commission toggles.json; do
              echo "Fetching $name ..."
              curl -sSf -H "X-Worker-Token: $WORKER_TOKEN" \
                   "$WORKER_URL/artifact/$JOB_ID/$name" -o "inputs/$name" || true
            done
          else
            echo "No WORKER_URL/JOB_ID provided; using repo inputs/ files if present."
          fi
          # Ensure required files exist (po, pi, commission). Fail if missing.
          for req in po pi commission; do
            if [ ! -s "inputs/$req" ]; then
              echo "::error file=inputs/$req::missing/empty required input"
              exit 1
            fi
          done
          # Optional toggles.json default
          if [ ! -s inputs/toggles.json ]; then
            echo '{}' > inputs/toggles.json
          fi

      - name: Compute PO_HASH and check manifest
        shell: bash
        run: |
          set -euo pipefail
          HASH=$(cat inputs/po inputs/pi inputs/commission | sha256sum | awk '{print $1}')
          echo "PO_HASH=$HASH" | tee -a "$GITHUB_ENV"
          MANIFEST_PATH="reports/$HASH/manifest.json"
          if [ -f "$MANIFEST_PATH" ]; then
            echo "Manifest exists at $MANIFEST_PATH — skipping job."
            echo "SKIP_PIPELINE=1" >> "$GITHUB_ENV"
          else
            echo "SKIP_PIPELINE=0" >> "$GITHUB_ENV"
          fi

      - name: Run PO review pipeline (parse, diff, build report)
        if: env.SKIP_PIPELINE != '1'
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p run

          # 1) Normalize & partition
          python scripts/normalize_and_partition.py \
            --inputs inputs/po inputs/pi inputs/commission \
            --out elements.json

          # 2) Chunk + index
          python scripts/chunk_and_index.py \
            --elements elements.json \
            --outdir run

          # 3) Deterministic checks
          python scripts/deterministic_checks.py \
            --elements elements.json \
            --template irr_main \
            --toggles inputs/toggles.json \
            --out run/deterministic.json

          # 4) Retrieve candidates
          python scripts/retrieve_candidates.py \
            --child_chunks run/child_chunks.jsonl \
            --requirements schemas/requirements_contract_main_IRR.json \
            --out run/llm_jobs.jsonl

          # 5) Optional classifier pass
          if [ -f classifier_model.joblib ]; then
            python scripts/classifier_infer.py \
              --model classifier_model.joblib \
              --input run/llm_jobs.jsonl \
              --output run/classified.jsonl
          fi

          # 6) (Placeholder) LLM call — dummy results to keep pipeline functional
          python - <<'PY'
import json, pathlib
pathlib.Path('run/llm_results.jsonl').write_text('', encoding='utf-8')
with open('run/llm_jobs.jsonl','r',encoding='utf-8') as f_in, \
     open('run/llm_results.jsonl','w',encoding='utf-8') as f_out:
    for line in f_in:
        job = json.loads(line)
        f_out.write(json.dumps({
            "clause": job.get("clause_id"),
            "status": "UNCERTAIN",
            "expected": job.get("expected"),
            "actual": "",
            "evidence": {"doc": "", "page": 0, "element_id": "", "excerpt": ""},
            "fix": "",
            "severity": "MEDIUM"
        }, ensure_ascii=False) + "\n")
PY

          # 7) Judge
          python scripts/judge.py \
            --schema schemas/structured_output_schema.json \
            --llm_results run/llm_results.jsonl \
            --template_text "{}" \
            --out run/judged.jsonl

          # 8) Build final report (expects to output below files in ./run)
          python scripts/report_builder.py \
            --deterministic run/deterministic.json \
            --judged run/judged.jsonl \
            --outdir run

          # Sanity: ensure outputs exist
          test -f run/00_Review_Report.docx
          test -f run/issues.csv
          test -f run/issues.json

      - name: Prepare report and diff artifacts
        if: env.SKIP_PIPELINE != '1'
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "reports/$PO_HASH"
          cp run/00_Review_Report.docx "reports/$PO_HASH/report.docx"
          cp run/issues.csv "reports/$PO_HASH/diff.csv"
          cp run/issues.json "reports/$PO_HASH/diff.json"

          # Build a simple HTML table from CSV (no external deps)
          python - <<'PY'
import os, csv, html
po_hash = os.environ["PO_HASH"]
src = f"reports/{po_hash}/diff.csv"
dst = f"reports/{po_hash}/diff.html"
rows = []
with open(src, newline='', encoding='utf-8') as f:
    rows = list(csv.reader(f))
def cell(x): return f"<td>{html.escape(x)}</td>"
def tr(xs): return "<tr>" + "".join(cell(s) for s in xs) + "</tr>"
thead = "<thead>" + tr(rows[0]) + "</thead>" if rows else ""
tbody = "<tbody>" + "".join(tr(r) for r in rows[1:]) + "</tbody>" if len(rows)>1 else "<tbody></tbody>"
doc = f"<!doctype html><meta charset='utf-8'><title>Diff</title><table>{thead}{tbody}</table>"
with open(dst, "w", encoding="utf-8") as out:
    out.write(doc)
PY

      - name: Create manifest.json
        if: env.SKIP_PIPELINE != '1'
        shell: bash
        run: |
          set -euo pipefail
          now=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          cat > "reports/$PO_HASH/manifest.json" <<JSON
{
  "status": "published",
  "timestamps": {
    "uploaded": "$now",
    "parsed": "$now",
    "diffed": "$now",
    "published": "$now"
  },
  "versions": {
    "pipeline_commit": "$(git rev-parse HEAD)"
  },
  "artifacts": {
    "report_docx": "reports/$PO_HASH/report.docx",
    "diff_csv": "reports/$PO_HASH/diff.csv",
    "diff_json": "reports/$PO_HASH/diff.json",
    "diff_html": "reports/$PO_HASH/diff.html"
  },
  "errors": []
}
JSON

      - name: Commit results and manifest
        if: env.SKIP_PIPELINE != '1'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add "reports/$PO_HASH"
          git commit -m "Add report & diffs for $PO_HASH [skip ci]" || exit 0
          git push

      - name: Upload outputs back to worker (optional)
        if: env.SKIP_PIPELINE != '1'
        env:
          WORKER_URL: ${{ secrets.WORKER_URL }}
          WORKER_TOKEN: ${{ secrets.WORKER_TOKEN }}
          JOB_ID: ${{ github.event.client_payload.jobId }}
        shell: bash
        run: |
          set -euo pipefail
          for file in 00_Review_Report.docx issues.json issues.csv; do
            if [ -f run/$file ]; then
              curl -sSf -H "X-Worker-Token: $WORKER_TOKEN" \
                   -X PUT --data-binary @run/$file \
                   "$WORKER_URL/artifact/$JOB_ID/$file"
            fi
          done
          echo '{"status":"ready"}' | \
            curl -sSf -H "X-Worker-Token: $WORKER_TOKEN" \
                 -X PUT --data-binary @- \
                 "$WORKER_URL/status/$JOB_ID"
